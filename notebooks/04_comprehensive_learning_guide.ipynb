{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Spoken Digit Recognition: The Complete Master Class\n",
                "\n",
                "Welcome! This notebook is designed to take you from \"zero\" to \"deep understanding\" of this codebase. We will cover:\n",
                "\n",
                "1.  **Software Engineering (SWE) Perspective**: Why is the code organized like this?\n",
                "2.  **Audio Signal Processing**: What is sound, really? How does the computer \"hear\"?\n",
                "3.  **The Machine Learning Pipeline**: From raw data to vectors.\n",
                "4.  **Deep Learning Theory**: How Convolutional Neural Networks (CNNs) work.\n",
                "5.  **The Process**: What we did, why it failed initially, and how we fixed it.\n",
                "\n",
                "---\n",
                "\n",
                "## Part 1: The Software Engineering (SWE) Foundation\n",
                "\n",
                "### Q: Why not just put everything in one big `main.py` file or a single notebook?\n",
                "**A:** Scalability, Reproducibility, and Sanity.\n",
                "\n",
                "In a professional **MLOps (Machine Learning Operations)** environment, we separate concerns:\n",
                "\n",
                "-   **`src/`**: The Core Logic. Code here is reusable. `src/data` handles loading, `src/models` handles architecture.\n",
                "-   **`notebooks/`**: The Laboratory. This is where we experiment, visualize, and fail fast. It uses the stable code from `src/`.\n",
                "-   **`requirements.txt`**: The Environment. Ensures your computer runs the same code as my computer.\n",
                "-   **`.gitignore`**: Hygiene. Tells Git: \"Ignore my local trash (compiled files, datasets, virtual envs). Only track the source code.\"\n",
                "\n",
                "This structure allows us to train a model in a script (`python src/models/train_model.py`) OR in a notebook (`03_training_and_opt.ipynb`) without rewriting code.\n",
                "\n",
                "---\n",
                "\n",
                "## Part 2: Audio Signal Processing (The \"Physics\" Layer)\n",
                "\n",
                "### What is Sound?\n",
                "Sound is just air vibrating. A microphone measures this vibration thousands of times per second. This is called the **Sample Rate**.\n",
                "-   **16,000 Hz (16kHz)**: Used in this project. We measure the air pressure 16,000 times every second.\n",
                "\n",
                "### The Problem: Raw Audio is Hard to Read\n",
                "A raw waveform is just a list of amplitutes. It's hard for a Neural Network to find patterns like \"pitch\" or \"tone\" in just a list of numbers.\n",
                "\n",
                "### The Solution: The Spectrogram\n",
                "We convert Time-Domain signals (Amplitude vs Time) to Frequency-Domain signals (Frequency vs Time).\n",
                "\n",
                "**Let's Visualize This:**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import os\n",
                "sys.path.append(os.path.abspath('../'))\n",
                "\n",
                "import torch\n",
                "import torchaudio\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "from src.data.dataset import SpokenDigitDataset\n",
                "\n",
                "# Load a sample\n",
                "dataset = SpokenDigitDataset('../data/processed')\n",
                "sample_file, label = dataset.file_list[0]\n",
                "waveform, sr = torchaudio.load(sample_file)\n",
                "\n",
                "# 1. The Waveform (Time Domain)\n",
                "plt.figure(figsize=(10, 4))\n",
                "plt.plot(waveform.t().numpy())\n",
                "plt.title(f\"Raw Waveform (Digit: {label})\")\n",
                "plt.xlabel(\"Time\")\n",
                "plt.ylabel(\"Amplitude\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### The Mel-Spectrogram\n",
                "We use `MelSpectrogram`. This is a spectrogram adjusted for human hearing (we hear low frequencies better than high ones).\n",
                "\n",
                "**Why this matters for the model:**\n",
                "Instead of processing a 1D line of data, the model now sees an **Image** (2D array: Time x Frequency). This allows us to use **CNNs**, which are great at analyzing images!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "mel_transform = torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=64)\n",
                "db_transform = torchaudio.transforms.AmplitudeToDB()\n",
                "\n",
                "spec = mel_transform(waveform)\n",
                "spec = db_transform(spec)\n",
                "\n",
                "plt.figure(figsize=(10, 4))\n",
                "plt.imshow(spec[0].numpy(), aspect='auto', origin='lower')\n",
                "plt.title(\"Mel-Spectrogram (What the Model Sees)\")\n",
                "plt.xlabel(\"Time\")\n",
                "plt.ylabel(\"Frequency (Mel)\")\n",
                "plt.colorbar(format='%+2.0f dB')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 3: Deep Learning & CNNs (The \"Brain\")\n",
                "\n",
                "We treat this audio problem as an **Image Classification** problem. We want to find the shape of \"Zero\" vs the shape of \"One\" on that heatmap above.\n",
                "\n",
                "### Key Components of our `DeeperCNN`:\n",
                "\n",
                "1.  **Conv2d (Convolution)**: A small filter (like a magnifying glass) slides over the image. It learns to detect edges, curves, or specific frequency patterns. We stack these to learn complex patterns.\n",
                "2.  **BatchNorm**: Keeps the math stable. It normalizes inputs so no single unexpected loud noise throws off the whole network.\n",
                "3.  **ReLU (Activation)**: Allows the network to learn non-linear patterns. Without this, the network is just a linear regression.\n",
                "4.  **MaxPool**: Downsamples the image. It says, \"I saw a feature here, I don't care exactly which pixel, just that it exists in this region.\"\n",
                "5.  **Dropout**: Randomly turns off neurons during training. This forces the network to be redundant and robust (prevents overfitting).\n",
                "\n",
                "---\n",
                "\n",
                "## Part 4: The Process (What we did)\n",
                "\n",
                "### Phase 1: The Baseline (`SimpleCNN`)\n",
                "We started with a small model (30 Epochs). It got ~87% accuracy.\n",
                "**Issue:** It was \"underfitting\". It didn't have enough \"brain capacity\" (neurons) to learn the subtle differences between digits like 1 and 7 in Afaan Oromoo.\n",
                "\n",
                "### Phase 2: Optimization (`DeeperCNN` + SpecAugment)\n",
                "1.  **More Layers**: We went from 16->32 channels to 32->64->128->200+ channels. This gave the model more capacity.\n",
                "2.  **SpecAugment**: We randomly erased blocks of time or frequency during training. This forced the model to learn context. \"If I can't hear the start of the word, can I guess it from the end?\"\n",
                "\n",
                "**Result**: 91.94% Accuracy.\n",
                "\n",
                "### Phase 3: The Ceiling\n",
                "We ran a Grid Search (trying many learning rates). The results saturated at ~92%. This suggests we are limited by the **Data**, not the model. To improve further, we would need thousands more samples, or to use **Transfer Learning** (using a model pre-trained on Google/Facebook data).\n",
                "\n",
                "---\n",
                "\n",
                "## Part 5: Future Steps (What YOU should do)\n",
                "\n",
                "1.  **Collect Data**: The best code cannot fix bad/small data. Record yourself saying digits 100 times and add it to the folders.\n",
                "2.  **Transfer Learning**: Look into `Wav2Vec 2.0`. It's a transformer model (like GPT) for audio.\n",
                "3.  **Deploy**: You already have `app.py`. Host it on Streamlit Cloud or HuggingFace Spaces to share directly with users."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}